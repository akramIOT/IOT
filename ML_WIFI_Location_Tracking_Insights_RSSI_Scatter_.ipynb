{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_WIFI_Location_Tracking_Insights_RSSI_Scatter .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFTJVK7rx7DKLSwkM6iSJb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akramIOT/IOT/blob/master/ML_WIFI_Location_Tracking_Insights_RSSI_Scatter_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHPQAMV0tJi0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "715dee90-fc2a-4824-f955-d6b68df9924d"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "!git clone --branch r1.13.0 --depth 1 https://github.com/tensorflow/models\n",
        "!pip install scapy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc4\n",
            "fatal: destination path 'models' already exists and is not an empty directory.\n",
            "Requirement already satisfied: scapy in /usr/local/lib/python3.6/dist-packages (2.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moltUI5X48Be",
        "colab_type": "code",
        "outputId": "68a6cc1e-62cb-4f5e-d9bc-c40d939de53d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## My environment is having Tensorflow 2.0 and Python 3.7 Installed, along with other frameworks.\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gym\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "dataset = pd.read_csv(\"/content/trainingData.csv\",header = 0)\n",
        "\n",
        "features = np.asarray(dataset.iloc[:,0:520])\n",
        "features[features == 100] = -110\n",
        "features = (features - features.mean()) / features.var()\n",
        "\n",
        "labels = np.asarray(dataset[\"BUILDINGID\"].map(str) + dataset[\"FLOOR\"].map(str))\n",
        "labels = np.asarray(pd.get_dummies(labels))\n",
        "\n",
        "\n",
        "train_val_split = np.random.rand(len(features)) < 0.70\n",
        "train_x = features[train_val_split]\n",
        "train_y = labels[train_val_split]\n",
        "val_x = features[~train_val_split]\n",
        "val_y = labels[~train_val_split]\n",
        "\n",
        "\n",
        "test_dataset = pd.read_csv(\"/content/validationData.csv\",header = 0)\n",
        "\n",
        "test_features = np.asarray(test_dataset.iloc[:,0:520])\n",
        "test_features[test_features == 100] = -110\n",
        "test_features = (test_features - test_features.mean()) / test_features.var()\n",
        "\n",
        "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
        "test_labels = np.asarray(pd.get_dummies(test_labels))\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.0, shape = shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "n_input = 520 \n",
        "n_hidden_1 = 256 \n",
        "n_hidden_2 = 128 \n",
        "n_hidden_3 = 64 \n",
        "\n",
        "n_classes = labels.shape[1]\n",
        "\n",
        "learning_rate = 0.00001\n",
        "training_epochs = 30\n",
        "batch_size = 15\n",
        "\n",
        "total_batches = train_x.shape[0] // batch_size\n",
        "\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,n_input])\n",
        "Y = tf.placeholder(tf.float32,[None,n_classes])\n",
        "\n",
        "# --------------------- Encoder Variables --------------- #\n",
        "\n",
        "e_weights_h1 = weight_variable([n_input, n_hidden_1])\n",
        "e_biases_h1 = bias_variable([n_hidden_1])\n",
        "\n",
        "e_weights_h2 = weight_variable([n_hidden_1, n_hidden_2])\n",
        "e_biases_h2 = bias_variable([n_hidden_2])\n",
        "\n",
        "e_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
        "e_biases_h3 = bias_variable([n_hidden_3])\n",
        "\n",
        "# --------------------- Decoder Variables --------------- #\n",
        "\n",
        "d_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
        "d_biases_h1 = bias_variable([n_hidden_2])\n",
        "\n",
        "d_weights_h2 = weight_variable([n_hidden_2, n_hidden_1])\n",
        "d_biases_h2 = bias_variable([n_hidden_1])\n",
        "\n",
        "d_weights_h3 = weight_variable([n_hidden_1, n_input])\n",
        "d_biases_h3 = bias_variable([n_input])\n",
        "\n",
        "# --------------------- DNN Variables ------------------ #\n",
        "\n",
        "dnn_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
        "dnn_biases_h1 = bias_variable([n_hidden_2])\n",
        "\n",
        "dnn_weights_h2 = weight_variable([n_hidden_2, n_hidden_2])\n",
        "dnn_biases_h2 = bias_variable([n_hidden_2])\n",
        "\n",
        "dnn_weights_out = weight_variable([n_hidden_2, n_classes])\n",
        "dnn_biases_out = bias_variable([n_classes])\n",
        "\n",
        "\n",
        "def encode(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,e_weights_h1),e_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,e_weights_h2),e_biases_h2))\n",
        "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,e_weights_h3),e_biases_h3))\n",
        "    return l3\n",
        "    \n",
        "def decode(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,d_weights_h1),d_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,d_weights_h2),d_biases_h2))\n",
        "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,d_weights_h3),d_biases_h3))\n",
        "    return l3\n",
        "\n",
        "def dnn(x):\n",
        "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,dnn_weights_h1),dnn_biases_h1))\n",
        "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,dnn_weights_h2),dnn_biases_h2))\n",
        "    out = tf.nn.softmax(tf.add(tf.matmul(l2,dnn_weights_out),dnn_biases_out))\n",
        "    return out\n",
        "\n",
        "encoded = encode(X)\n",
        "decoded = decode(encoded) \n",
        "y_ = dnn(encoded)\n",
        "\n",
        "us_cost_function = tf.reduce_mean(tf.pow(X - decoded, 2))\n",
        "s_cost_function = -tf.reduce_sum(Y * tf.log(y_))\n",
        "us_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(us_cost_function)\n",
        "s_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(s_cost_function)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "with tf.Session() as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    \n",
        "    # ------------ 1. Training Autoencoders - Unsupervised Learning ----------- #\n",
        "    for epoch in range(training_epochs):\n",
        "        epoch_costs = np.empty(0)\n",
        "        for b in range(total_batches):\n",
        "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
        "            batch_x = train_x[offset:(offset + batch_size), :]\n",
        "            _, c = session.run([us_optimizer, us_cost_function],feed_dict={X: batch_x})\n",
        "            epoch_costs = np.append(epoch_costs,c)\n",
        "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs))\n",
        "    print(\"Unsupervised pre-training finished...\")\n",
        "    \n",
        "    \n",
        "    # ---------------- 2. Training NN - Supervised Learning ------------------ #\n",
        "    for epoch in range(training_epochs):\n",
        "        epoch_costs = np.empty(0)\n",
        "        for b in range(total_batches):\n",
        "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
        "            batch_x = train_x[offset:(offset + batch_size), :]\n",
        "            batch_y = train_y[offset:(offset + batch_size), :]\n",
        "            _, c = session.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
        "            epoch_costs = np.append(epoch_costs,c)\n",
        "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Training Accuracy: \", \\\n",
        "            session.run(accuracy, feed_dict={X: train_x, Y: train_y}), \\\n",
        "            \"Validation Accuracy:\", session.run(accuracy, feed_dict={X: val_x, Y: val_y}))\n",
        "            \n",
        "    print(\"Supervised training finished...\")\n",
        "    \n",
        "\n",
        "    print(\"\\nTesting Accuracy:\", session.run(accuracy, feed_dict={X: test_features, Y: test_labels}))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss:  0.06756741723669261\n",
            "Epoch:  1  Loss:  0.043572238630407656\n",
            "Epoch:  2  Loss:  0.03319095554325501\n",
            "Epoch:  3  Loss:  0.026805706440488155\n",
            "Epoch:  4  Loss:  0.022664126235815277\n",
            "Epoch:  5  Loss:  0.01985769359238518\n",
            "Epoch:  6  Loss:  0.017764222261318446\n",
            "Epoch:  7  Loss:  0.016128599234687242\n",
            "Epoch:  8  Loss:  0.014815651394118053\n",
            "Epoch:  9  Loss:  0.013760425510793218\n",
            "Epoch:  10  Loss:  0.01291333131574343\n",
            "Epoch:  11  Loss:  0.012217938067668205\n",
            "Epoch:  12  Loss:  0.011626974573765908\n",
            "Epoch:  13  Loss:  0.011108639032005527\n",
            "Epoch:  14  Loss:  0.010644274199355406\n",
            "Epoch:  15  Loss:  0.010224499407628365\n",
            "Epoch:  16  Loss:  0.009844709591001706\n",
            "Epoch:  17  Loss:  0.00950151951021171\n",
            "Epoch:  18  Loss:  0.009191890124109273\n",
            "Epoch:  19  Loss:  0.008913523931080322\n",
            "Epoch:  20  Loss:  0.008664405491429635\n",
            "Epoch:  21  Loss:  0.008441878458890018\n",
            "Epoch:  22  Loss:  0.008242499193304947\n",
            "Epoch:  23  Loss:  0.008062634057695181\n",
            "Epoch:  24  Loss:  0.007899048433170942\n",
            "Epoch:  25  Loss:  0.007749146267579241\n",
            "Epoch:  26  Loss:  0.007610946916498894\n",
            "Epoch:  27  Loss:  0.007482957047539632\n",
            "Epoch:  28  Loss:  0.007364028688335733\n",
            "Epoch:  29  Loss:  0.0072532352403435855\n",
            "Unsupervised pre-training finished...\n",
            "Epoch:  0  Loss:  35.43869089592645  Training Accuracy:  0.50789094 Validation Accuracy: 0.5037519\n",
            "Epoch:  1  Loss:  28.928328214343576  Training Accuracy:  0.60078907 Validation Accuracy: 0.60630316\n",
            "Epoch:  2  Loss:  23.150513584313533  Training Accuracy:  0.6687948 Validation Accuracy: 0.6835084\n",
            "Epoch:  3  Loss:  18.887739523363063  Training Accuracy:  0.7534433 Validation Accuracy: 0.76288146\n",
            "Epoch:  4  Loss:  15.789280654026438  Training Accuracy:  0.81169295 Validation Accuracy: 0.81457394\n",
            "Epoch:  5  Loss:  13.386807287470774  Training Accuracy:  0.8484218 Validation Accuracy: 0.85259295\n",
            "Epoch:  6  Loss:  11.422902947478196  Training Accuracy:  0.88737446 Validation Accuracy: 0.88711023\n",
            "Epoch:  7  Loss:  9.785321086035607  Training Accuracy:  0.9159254 Validation Accuracy: 0.9162915\n",
            "Epoch:  8  Loss:  8.410275610386362  Training Accuracy:  0.9348637 Validation Accuracy: 0.934634\n",
            "Epoch:  9  Loss:  7.247168520615868  Training Accuracy:  0.94748926 Validation Accuracy: 0.9476405\n",
            "Epoch:  10  Loss:  6.2559620781016685  Training Accuracy:  0.95681494 Validation Accuracy: 0.958646\n",
            "Epoch:  11  Loss:  5.40702535534444  Training Accuracy:  0.9643472 Validation Accuracy: 0.9668167\n",
            "Epoch:  12  Loss:  4.678284916669869  Training Accuracy:  0.96901006 Validation Accuracy: 0.9718192\n",
            "Epoch:  13  Loss:  4.052537701165843  Training Accuracy:  0.97166425 Validation Accuracy: 0.9768217\n",
            "Epoch:  14  Loss:  3.515769613860626  Training Accuracy:  0.9748207 Validation Accuracy: 0.97982323\n",
            "Epoch:  15  Loss:  3.056192086365944  Training Accuracy:  0.97704446 Validation Accuracy: 0.98082376\n",
            "Epoch:  16  Loss:  2.6636183952393395  Training Accuracy:  0.9786944 Validation Accuracy: 0.9823245\n",
            "Epoch:  17  Loss:  2.329038969586945  Training Accuracy:  0.98156387 Validation Accuracy: 0.98482573\n",
            "Epoch:  18  Loss:  2.044379786394769  Training Accuracy:  0.9827116 Validation Accuracy: 0.985326\n",
            "Epoch:  19  Loss:  1.802403077515257  Training Accuracy:  0.9844333 Validation Accuracy: 0.98716027\n",
            "Epoch:  20  Loss:  1.5966889300294405  Training Accuracy:  0.98515064 Validation Accuracy: 0.98766047\n",
            "Epoch:  21  Loss:  1.4216298409792785  Training Accuracy:  0.98629844 Validation Accuracy: 0.9889945\n",
            "Epoch:  22  Loss:  1.27239715583839  Training Accuracy:  0.9877331 Validation Accuracy: 0.989328\n",
            "Epoch:  23  Loss:  1.1448747098718572  Training Accuracy:  0.98888093 Validation Accuracy: 0.9898282\n",
            "Epoch:  24  Loss:  1.0355745868514763  Training Accuracy:  0.98952657 Validation Accuracy: 0.9903285\n",
            "Epoch:  25  Loss:  0.9415537844681702  Training Accuracy:  0.9902439 Validation Accuracy: 0.9901618\n",
            "Epoch:  26  Loss:  0.8603375635487784  Training Accuracy:  0.9911765 Validation Accuracy: 0.9901618\n",
            "Epoch:  27  Loss:  0.7898559267076276  Training Accuracy:  0.9915351 Validation Accuracy: 0.99049526\n",
            "Epoch:  28  Loss:  0.728386714986342  Training Accuracy:  0.9915351 Validation Accuracy: 0.9901618\n",
            "Epoch:  29  Loss:  0.6745068804993504  Training Accuracy:  0.99175036 Validation Accuracy: 0.990662\n",
            "Supervised training finished...\n",
            "\n",
            "Testing Accuracy: 0.9108911\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}